from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import torch
import json
import os
from PIL import Image
from tqdm import tqdm
import multiprocessing as mp
from functools import partial
import argparse
from pathlib import Path

# =====================================================
# åŸºç¡€é…ç½®
# =====================================================
benchmark_paths = [
    "benchmark/pointing.json",
    "benchmark/freespace.json",
    "benchmark/color.json",
    "benchmark/counting.json",
    "benchmark/distance.json",
    "benchmark/height.json",
    "benchmark/bbox.json",
    "benchmark/function.json",
    "benchmark/landing.json",
    "benchmark/reverse.json",
    "benchmark/single_image_caption.json",
    "benchmark/spatial.json",
    "benchmark/multi_image_caption.json",
]

max_pixels = 1605632
min_pixels = 256 * 28 * 28

gen_args = {
    "temperature": 0,
    "top_p": 1,
    "max_new_tokens": 1024,
    "repetition_penalty": 1.05,
    "do_sample": False,
}

use_flash_attention = False
save_every = 10

NUM_GPUS = 8  # æ€»GPUæ•°é‡


# =====================================================
# è¾…åŠ©å‡½æ•°ï¼šä»ckptè·¯å¾„æå–åä¸¤ä¸ªç›®å½•ä½œä¸ºè¾“å‡ºè·¯å¾„
# =====================================================
def get_output_dir_from_ckpt(ckpt_path):
    """
    ä»checkpointè·¯å¾„æå–åä¸¤ä¸ªç›®å½•å
    ä¾‹å¦‚: /path/to/cvpr-rl_cvpr_only_freespace_avg_20251108-1948/global_step_20
    è¿”å›: cvpr-rl_cvpr_only_freespace_avg_20251108-1948/global_step_20
    """
    path = Path(ckpt_path)
    # è·å–æœ€åä¸¤ä¸ªç›®å½•
    if len(path.parts) >= 2:
        output_dir = os.path.join(path.parts[-2], path.parts[-1])
    else:
        # å¦‚æœè·¯å¾„ä¸å¤Ÿé•¿ï¼Œä½¿ç”¨basename
        output_dir = path.name
    return output_dir


def get_model_name_from_ckpt(ckpt_path):
    """
    ä»checkpointè·¯å¾„æå–æ¨¡å‹åç§°
    ä¾‹å¦‚: global_step_20 -> step20
    """
    path = Path(ckpt_path)
    step_name = path.name  # ä¾‹å¦‚: global_step_20
    if "step" in step_name.lower():
        # æå–æ•°å­—éƒ¨åˆ†
        import re
        match = re.search(r'step[_-]?(\d+)', step_name, re.IGNORECASE)
        if match:
            return f"step{match.group(1)}"
    return step_name


# =====================================================
# å•ä¸ªGPUå¤„ç†å‡½æ•°
# =====================================================
def process_benchmark_on_gpu(gpu_id, benchmark_path, model_path, output_dir, model_name):
    """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†å•ä¸ªbenchmark"""
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GPUï¼ˆä¸è¦ç”¨CUDA_VISIBLE_DEVICESï¼‰
    torch.cuda.set_device(gpu_id)
    device = f"cuda:{gpu_id}"
    
    filename = os.path.basename(benchmark_path)
    prefix = filename.split("_")[0]
    output_file = os.path.join(output_dir, f"{prefix}_{model_name}.json")
    
    print(f"\n[GPU {gpu_id}] Processing Task: {prefix}")
    print(f"[GPU {gpu_id}] Benchmark path: {benchmark_path}")
    
    # åŠ è½½æ¨¡å‹
    print(f"[GPU {gpu_id}] Loading model...")
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map={"": device},
        attn_implementation="flash_attention_2" if use_flash_attention else None,
    )
    processor = AutoProcessor.from_pretrained(model_path, max_pixels=max_pixels, min_pixels=min_pixels)
    print(f"[GPU {gpu_id}] Model loaded successfully on {device}.")
    
    # åŠ è½½ benchmark æ•°æ®
    try:
        with open(benchmark_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)
    except Exception as e:
        print(f"[GPU {gpu_id}] Failed to load {benchmark_path}: {e}")
        return
    
    if not isinstance(dataset, list):
        print(f"[GPU {gpu_id}] Invalid data format in {benchmark_path}, expected a list.")
        return
    
    results = []
    
    # è¿›åº¦æ¡
    progress_bar = tqdm(
        enumerate(dataset),
        total=len(dataset),
        desc=f"[GPU {gpu_id}][{prefix}]",
        position=gpu_id,
        dynamic_ncols=True
    )
    
    # æ¨ç†å¾ªç¯
    for idx, sample in progress_bar:
        images_paths = sample.get("images", None)
        if images_paths is None:
            images_paths = [sample.get("image", "")]
        
        question = sample.get("question", "")
        ground_truth = sample.get("ground_truth", "")
        
        loaded_images = []
        skip_sample = False
        for img_path in images_paths:
            if not os.path.exists(img_path):
                progress_bar.write(f"[GPU {gpu_id}][Warning] Image not found: {img_path}")
                skip_sample = True
                break
            try:
                img = Image.open(img_path).convert("RGB")
                loaded_images.append(img)
            except Exception as e:
                progress_bar.write(f"[GPU {gpu_id}][Warning] Failed to open image {img_path}: {e}")
                skip_sample = True
                break
        if skip_sample:
            continue
        
        # æ„é€ è¾“å…¥æ¶ˆæ¯
        image_messages = [{"type": "image", "image": img} for img in loaded_images]
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": image_messages + [{"type": "text", "text": question}]},
        ]
        
        # åº”ç”¨æ¨¡æ¿
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = processor(text=[text], images=loaded_images, return_tensors="pt", padding=True).to(device)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            generated_ids = model.generate(**inputs, **gen_args)
        
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        results.append({
            "image": images_paths,
            "question": question,
            "predicted_answer": output_text,
            "ground_truth": ground_truth,
        })
        
        # æ¯éš” save_every ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (idx + 1) % save_every == 0:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
        
        progress_bar.set_postfix({"saved": len(results)})
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=4)
    
    progress_bar.close()
    print(f"[GPU {gpu_id}] âœ… Task {prefix} complete. Results saved to: {output_file}")
    
    # æ¸…ç†GPUå†…å­˜
    del model
    del processor
    torch.cuda.empty_cache()


# =====================================================
# ä¸»ç¨‹åºï¼šåˆ†æ‰¹å¤„ç†
# =====================================================
if __name__ == "__main__":
    # è®¾ç½®multiprocessingå¯åŠ¨æ–¹æ³•ä¸ºspawnï¼ˆCUDAè¦æ±‚ï¼‰
    mp.set_start_method('spawn', force=True)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="Multi-GPU Parallel Inference for Vision-Language Model")
    parser.add_argument(
        "--ckpt_path",
        type=str,
        required=True,
        help="Path to the checkpoint directory (e.g., /path/to/cvpr-rl_xxx/global_step_20)"
    )
    parser.add_argument(
        "--num_gpus",
        type=int,
        default=8,
        help="Number of GPUs to use (default: 8)"
    )
    parser.add_argument(
        "--output_base_dir",
        type=str,
        default=None,
        help="Base directory for output (default: auto-generated from ckpt_path)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    model_path = args.ckpt_path
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºç›®å½•
    if args.output_base_dir:
        output_dir = args.output_base_dir
    else:
        output_dir = get_output_dir_from_ckpt(model_path)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹åç§°
    model_name = get_model_name_from_ckpt(model_path)
    
    # æ›´æ–°GPUæ•°é‡
    num_gpus = args.num_gpus
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("=" * 80)
    print("Configuration:")
    print(f"  Checkpoint Path: {model_path}")
    print(f"  Output Directory: {output_dir}")
    print(f"  Model Name: {model_name}")
    print(f"  Number of GPUs: {num_gpus}")
    print(f"  Total Benchmarks: {len(benchmark_paths)}")
    print("=" * 80)
    
    # ç¬¬ä¸€æ‰¹ï¼šå‰num_gpusä¸ªbenchmarkåˆ†é…åˆ°num_gpusä¸ªGPU
    batch1 = benchmark_paths[:num_gpus]
    # ç¬¬äºŒæ‰¹ï¼šå‰©ä½™benchmarkåˆ†é…åˆ°å¯¹åº”æ•°é‡çš„GPU
    batch2 = benchmark_paths[num_gpus:]
    
    print(f"\nBatch 1: {len(batch1)} benchmarks on {num_gpus} GPUs")
    print(f"Batch 2: {len(batch2)} benchmarks on {min(len(batch2), num_gpus)} GPUs")
    print("=" * 80)
    
    # å¤„ç†ç¬¬ä¸€æ‰¹
    print("\nğŸš€ Starting Batch 1...")
    processes = []
    for gpu_id, benchmark_path in enumerate(batch1):
        p = mp.Process(
            target=process_benchmark_on_gpu,
            args=(gpu_id, benchmark_path, model_path, output_dir, model_name)
        )
        p.start()
        processes.append(p)
    
    # ç­‰å¾…ç¬¬ä¸€æ‰¹å®Œæˆ
    for p in processes:
        p.join()
    
    print("\nâœ… Batch 1 completed!")
    
    # å¤„ç†ç¬¬äºŒæ‰¹
    if batch2:
        print("\nğŸš€ Starting Batch 2...")
        processes = []
        for gpu_id, benchmark_path in enumerate(batch2):
            p = mp.Process(
                target=process_benchmark_on_gpu,
                args=(gpu_id, benchmark_path, model_path, output_dir, model_name)
            )
            p.start()
            processes.append(p)
        
        # ç­‰å¾…ç¬¬äºŒæ‰¹å®Œæˆ
        for p in processes:
            p.join()
        
        print("\nâœ… Batch 2 completed!")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ All tasks completed successfully!")
    print(f"ğŸ“ Results saved in: {output_dir}")
    print("=" * 80)
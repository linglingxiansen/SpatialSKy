import os
import json
from tqdm import tqdm
from openai import OpenAI

# =========================================================
# åŸºç¡€é…ç½®
# =========================================================
API_KEY = "your api key"
API_BASE_URL = "your api base url"

client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE_URL,
    default_headers={"X-Model-Provider-Id": "azure_openai"}
)

# =========================================================
# æ¨¡å‹æ‰“åˆ†å‡½æ•°
# =========================================================
def score_prediction(question, predicted_answer, ground_truth):
    """
    ä½¿ç”¨æ¨¡å‹å¯¹é¢„æµ‹ç­”æ¡ˆè¿›è¡Œ1-10åˆ†çš„è¯„åˆ†ã€‚
    """

    prompt = f"""
You are an expert UAV (drone) safety evaluator. 
You are given:
1ï¸âƒ£ The **Question** (what the model was asked)
2ï¸âƒ£ The **Predicted Answer** (the modelâ€™s output)
3ï¸âƒ£ The **Ground Truth** (the correct reference answer)

Please carefully compare the predicted answer with the ground truth.

Rate the predicted answer **strictly from 1 to 10** based on:
- Accuracy of safety assessment
- Correctness and completeness of key elements (hazards, landing feasibility, etc.)
- Consistency with ground truth
- Usefulness and factual precision

Output **only JSON**:
{{
  "score": <integer from 1 to 10>,
  "reason": "short explanation (1-2 sentences)"
}}

---
Question:
{question}

Predicted Answer:
{predicted_answer}

Ground Truth:
{json.dumps(ground_truth, indent=2, ensure_ascii=False)}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "You are a strict evaluator of UAV safety reports."},
                {"role": "user", "content": prompt},
            ],
        )
        content = response.choices[0].message.content.strip()
        result = json.loads(content)
        return result.get("score", None)
    except Exception as e:
        print(f"[Error Scoring] {e}")
        return None


# =========================================================
# ä¸»ç¨‹åº
# =========================================================
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='è®¡ç®— Landing QA çš„æ¨¡å‹è¯„åˆ†')
    parser.add_argument('--file', type=str, required=True, help='è¦è¯„ä¼°çš„ JSON æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    file_path = args.file
    if not os.path.exists(file_path):
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    filename = os.path.basename(file_path)
    print(f"\nğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"[Error Loading JSON] {file_path}: {e}")
        return

    # å¤„ç†å½“æ–‡ä»¶ä¸º dict (id->sample) æƒ…å†µï¼Œè½¬æˆ list
    if isinstance(data, dict):
        items = list(data.values())
    elif isinstance(data, list):
        items = data
    else:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œå¿…é¡»æ˜¯ list æˆ– dict")
        return

    scores = []
    for item in tqdm(items, desc=f"Scoring {filename}", leave=False):
        q = item.get("question", "")
        p = item.get("predicted_answer", "")
        g = item.get("ground_truth", "")

        score = score_prediction(q, p, g)
        if score is not None:
            scores.append(score)

    if scores:
        avg = sum(scores) / len(scores)
        print(f"âœ… {filename} å¹³å‡å¾—åˆ†: {avg:.2f}/10 ({len(scores)}/{len(items)} ä¸ªæœ‰æ•ˆè¯„åˆ†)")
    else:
        print(f"âš ï¸ {filename} æ— æœ‰æ•ˆè¯„åˆ†ç»“æœã€‚")


if __name__ == "__main__":
    main()
